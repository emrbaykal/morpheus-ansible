# Role 08: ubuntu-kubernetes-join-node

## Overview

Joins worker nodes to an existing Kubernetes cluster using the `kubeadm join` command. The join command and token are dynamically retrieved from Morpheus, where they were previously stored by the `get-join-command.py` helper script.

---

## Purpose

After the Kubernetes control plane is initialized (role 07), worker nodes must join the cluster to provide compute capacity for workloads. This role automates the join process for each worker node, consuming the pre-generated join command stored in Morpheus as a result variable.

The role is designed to run on worker nodes only and includes an existence check to ensure it does not attempt to re-join a node that is already a member of the cluster.

---

## What It Does

The role performs the following tasks in order:

1. **Checks if the node has already joined**: Inspects whether `/etc/kubernetes/kubelet.conf` exists on the target host. This file is created by `kubeadm join` upon successful cluster membership. If it already exists, the join task is skipped.

2. **Executes the join command**: If `/etc/kubernetes/kubelet.conf` does not exist, runs the full `kubeadm join` command that was previously generated by the control plane and stored in `morpheus['results']['k8getjoin']`. This command includes:
   - The API server endpoint (control plane IP and port 6443)
   - The bootstrap token
   - The CA certificate hash for secure node verification

3. **Waits 60 seconds**: After the join command completes, pauses for 60 seconds to allow `kubelet` to start, the node to register with the API server, and initial pod scheduling to begin.

---

## Role Directory Structure

```
roles/08-ubuntu-kubernetes-join-node/
├── README.md               # This file
├── defaults/
│   └── main.yml            # Default variable values
├── handlers/
│   └── main.yml            # No handlers defined for this role
├── meta/
│   └── main.yml            # Role metadata and dependencies
└── tasks/
    └── main.yml            # Task definitions
```

---

## Variables

| Variable | Required | Source | Example | Description |
|----------|----------|--------|---------|-------------|
| `k8getjoin` | Yes | Morpheus result variable | `kubeadm join 192.168.1.10:6443 --token abc123 --discovery-token-ca-cert-hash sha256:xyz` | The complete `kubeadm join` command generated by `get-join-command.py` and stored in Morpheus. |

### Morpheus Integration

```
morpheus['results']['k8getjoin']   →  The full kubeadm join command string
morpheus['instance']['name']       →  Used to identify which node is the control plane (not this node)
```

The join command is generated by the `scripts/get-join-command.py` Python helper script, which:
1. SSH connects to the control plane node
2. Retrieves the join command via `kubeadm token create --print-join-command`
3. Returns the result to Morpheus, making it available as `morpheus['results']['k8getjoin']`
4. Retries for up to 3 minutes if the control plane is not yet ready

---

## Execution Guard

This role should be run on worker nodes only. In Morpheus, the playbook targeting typically ensures this by running on the node being provisioned, which is not the control plane. As an additional safeguard, the role may check that the hostname does not match `morpheus['instance']['name']` (which identifies the control plane).

---

## Dependencies

- **Role 07** must have completed on the control plane node before this role runs.
- `scripts/get-join-command.py` must have executed successfully and stored the join command in `morpheus['results']['k8getjoin']`.
- Requires `root` or `sudo` privileges (`become: true`).
- The worker node must be able to reach the control plane API server on port 6443.
- Roles 01-06 must have completed on the worker node.

---

## Playbook Usage

This role is executed by the following playbook:

**`ubuntu-k8-join-node.yml`** — Runs on worker nodes after the control plane is initialized.

Example playbook snippet:

```yaml
- hosts: k8s_workers
  become: true
  roles:
    - role: 08-ubuntu-kubernetes-join-node
```

---

## Verification Commands

After the role runs, verify the node has joined the cluster. Run these commands on the **control plane node**:

```bash
# Verify the new worker node appears in the cluster
kubectl get nodes
# Expected: The worker node should be listed with status Ready (may take a minute)

# Check node details
kubectl describe node <worker-node-name>

# Verify kubelet is running on the worker node (run on the worker)
systemctl status kubelet

# Check that kubelet.conf was created (run on the worker)
ls -la /etc/kubernetes/kubelet.conf

# Verify Flannel pod is running on the new node (run on control plane)
kubectl get pods -n kube-flannel -o wide | grep <worker-node-name>
```

---

## Notes and Caveats

- **Token expiry**: Bootstrap tokens expire after 24 hours. If the join command is used after the token has expired, `kubeadm join` will fail with an authentication error. Generate a new token with `kubeadm token create --print-join-command` on the control plane and update the Morpheus result variable.
- **Idempotency**: The check on `/etc/kubernetes/kubelet.conf` makes this role idempotent. Re-running the role on an already-joined node will skip the join command and the wait, producing no changes.
- **Network timing**: The 60-second wait after joining accounts for kubelet startup, CNI plugin initialization, and initial health check time. In environments with slow network storage or constrained resources, this may need to be extended.
- **CA hash verification**: The `--discovery-token-ca-cert-hash sha256:...` parameter in the join command cryptographically verifies the control plane's identity, protecting against man-in-the-middle attacks during node enrollment.
- **Multiple workers**: This role can be run simultaneously on multiple worker nodes. Each node independently executes its join command and registers with the API server. There is no coordination required between workers.
- The join command should be treated as a secret: it contains a valid bootstrap token that grants cluster access. Morpheus result variables should be protected with appropriate access controls.

